# -*- coding: utf-8 -*-
"""Untitled7.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1lezT5gCrUeE4f210jiUPlBvNrgajctRa
"""

#!/usr/bin/env python3
"""
MCUNet Model Training on Google Colab (CPU-Optimized Version)
====================================
This notebook provides a complete workflow for:
1. Setting up MCUNet in Google Colab
2. Training a model using TinyNAS with constraints for ESP32 WROVER
3. Exporting the model for ESP32 deployment
"""

# First, let's check PyTorch configuration
import torch
print(f"PyTorch version: {torch.__version__}")
print(f"CUDA available: {torch.cuda.is_available()}")
if torch.cuda.is_available():
    print(f"CUDA device: {torch.cuda.get_device_name(0)}")
else:
    print("Running on CPU only")

# Install required packages (only what's needed)
!pip install -q pytorch-lightning numpy matplotlib einops timm thop

# Skip installing mcunet since it's causing errors
print("Skipping mcunet installation to avoid dependency issues")

"""
Part 1: Dataset Preparation
==========================
We'll use CIFAR-10 for this example with optimized data handling for CPU
"""

import os
import numpy as np
import torch
import torchvision
import torchvision.transforms as transforms
from torch.utils.data import DataLoader, random_split

# Define data transformations
mean = (0.4914, 0.4822, 0.4465)
std = (0.2470, 0.2435, 0.2616)

train_transform = transforms.Compose([
    transforms.RandomCrop(32, padding=4),
    transforms.RandomHorizontalFlip(),
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

test_transform = transforms.Compose([
    transforms.ToTensor(),
    transforms.Normalize(mean, std),
])

# Download and prepare the dataset
train_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=True, download=True, transform=train_transform
)

test_dataset = torchvision.datasets.CIFAR10(
    root='./data', train=False, download=True, transform=test_transform
)

# Create data loaders optimized for CPU
# Smaller batch sizes and fewer workers for CPU
num_workers = 2  # Reduced workers
train_loader = DataLoader(
    train_dataset,
    batch_size=64,  # Smaller batch size for CPU
    shuffle=True,
    num_workers=num_workers
)

test_loader = DataLoader(
    test_dataset,
    batch_size=128,  # Smaller batch size for CPU
    shuffle=False,
    num_workers=num_workers
)

# Display some information about the dataset
print(f"Training set size: {len(train_dataset)}")
print(f"Test set size: {len(test_dataset)}")
print(f"Number of classes: {len(train_dataset.classes)}")
print(f"Classes: {train_dataset.classes}")

"""
Part 2: Define Resource Constraints for ESP32 WROVER
==================================================
Modified constraints to ensure models will fit
"""

# Define ESP32 WROVER constraints
ESP32_WROVER_CONSTRAINTS = {
    'flash_kb': 4 * 1024,  # 4MB Flash
    'sram_kb': 520,        # 520KB internal SRAM
    'psram_kb': 8 * 1024,  # 8MB PSRAM
    'freq_mhz': 240,       # 240MHz CPU frequency
    'fetch_per_cycle': 2,  # Instructions per cycle (approximate)
}

# Relaxed model constraints to ensure we can find a valid model
MODEL_CONSTRAINTS = {
    'model_size_kb': 400,     # Increased from 2*1024 to reduce complexity
    'activation_kb': 450,     # Increased from 200 to accommodate simpler models
    'latency_ms': 500,        # Target 500ms inference time
}

"""
Part 3: Set up TinyNAS for Neural Architecture Search (CPU-Optimized)
===================================================
This section implements a simpler version of TinyNAS optimized for CPU usage
"""

import sys
import time
from tqdm import tqdm
import torch.nn as nn
import torch.optim as optim
from functools import lru_cache
import torch.nn.functional as F

class TinyMobileBlock(nn.Module):
    """A very lightweight block for small models"""
    def __init__(self, in_channels, out_channels, stride=1):
        super(TinyMobileBlock, self).__init__()
        # Depthwise separable convolution to reduce parameters
        self.conv_dw = nn.Conv2d(in_channels, in_channels, 3, stride, 1, groups=in_channels, bias=False)
        self.bn_dw = nn.BatchNorm2d(in_channels)
        self.conv_pw = nn.Conv2d(in_channels, out_channels, 1, 1, 0, bias=False)
        self.bn_pw = nn.BatchNorm2d(out_channels)

    def forward(self, x):
        x = F.relu(self.bn_dw(self.conv_dw(x)))
        x = F.relu(self.bn_pw(self.conv_pw(x)))
        return x

class TinyNAS:
    def __init__(self, input_size=(3, 32, 32), num_classes=10, constraints=None):
        self.input_size = input_size
        self.num_classes = num_classes
        self.constraints = constraints
        self.best_model = None
        self.best_accuracy = 0.0

    def generate_tiny_architecture(self, complexity_level):
        """Generate a very lightweight architecture based on complexity level (0-5)"""
        # Minimal base filters to create smaller networks
        base_filters = 4 + complexity_level * 2

        class TinyNet(nn.Module):
            def __init__(self, base_filters, num_classes):
                super(TinyNet, self).__init__()
                # Initial convolution with fewer filters
                self.conv1 = nn.Conv2d(3, base_filters, kernel_size=3, stride=1, padding=1)
                self.bn1 = nn.BatchNorm2d(base_filters)

                # Single lightweight block with minimal filters
                self.block1 = TinyMobileBlock(base_filters, base_filters*2, stride=2)

                # Global pooling to reduce parameters
                self.pool = nn.AdaptiveAvgPool2d(1)
                self.classifier = nn.Linear(base_filters*2, num_classes)

            def forward(self, x):
                x = F.relu(self.bn1(self.conv1(x)))
                x = self.block1(x)
                x = self.pool(x)
                x = torch.flatten(x, 1)
                x = self.classifier(x)
                return x

        return TinyNet(base_filters, self.num_classes)

    def estimate_model_size(self, model):
        """Estimate the model size in KB"""
        param_size = 0
        for param in model.parameters():
            param_size += param.nelement() * param.element_size()

        buffer_size = 0
        for buffer in model.buffers():
            buffer_size += buffer.nelement() * buffer.element_size()

        total_size_kb = (param_size + buffer_size) / 1024
        return total_size_kb

    def estimate_activation_size(self, model, input_size):
        """Estimate peak activation memory in KB (simplified for CPU)"""
        device = next(model.parameters()).device
        x = torch.randn(1, *input_size, device=device)

        # Use forward hooks to capture activation sizes
        activation_sizes = []
        hooks = []

        def hook_fn(module, input, output):
            if isinstance(output, torch.Tensor):
                activation_sizes.append(output.nelement() * output.element_size())
            elif isinstance(output, tuple) and all(isinstance(o, torch.Tensor) for o in output):
                for o in output:
                    activation_sizes.append(o.nelement() * o.element_size())

        # Register hooks only on main modules
        for name, module in model.named_modules():
            if any(isinstance(module, t) for t in
                  [nn.Conv2d, nn.Linear, nn.BatchNorm2d, nn.MaxPool2d, nn.AdaptiveAvgPool2d]):
                hooks.append(module.register_forward_hook(hook_fn))

        # Forward pass with no gradient tracking
        with torch.no_grad():
            model(x)

        # Remove hooks
        for hook in hooks:
            hook.remove()

        # Calculate activation size in KB
        activation_size_kb = sum(activation_sizes) / 1024
        return activation_size_kb

    def evaluate_model(self, model, test_loader, device='cpu'):
        """Evaluate model accuracy on test set"""
        model.to(device)
        model.eval()
        correct = 0
        total = 0

        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = model(inputs)
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

        accuracy = 100.0 * correct / total
        return accuracy

    def train_model(self, model, train_loader, test_loader, epochs=5, device='cpu'):
        """Train the model (optimized for CPU)"""
        model.to(device)
        criterion = nn.CrossEntropyLoss()
        optimizer = optim.Adam(model.parameters(), lr=0.001, weight_decay=1e-4)
        scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=epochs)

        best_acc = 0.0

        for epoch in range(epochs):
            # Training
            model.train()
            train_loss = 0
            correct = 0
            total = 0

            pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs}")
            for inputs, targets in pbar:
                inputs, targets = inputs.to(device), targets.to(device)

                # Forward pass
                outputs = model(inputs)
                loss = criterion(outputs, targets)

                # Backward pass and optimization
                optimizer.zero_grad()
                loss.backward()
                optimizer.step()

                # Track metrics
                train_loss += loss.item()
                _, predicted = outputs.max(1)
                total += targets.size(0)
                correct += predicted.eq(targets).sum().item()

                # Update progress bar
                train_acc = 100.0 * correct / total
                pbar.set_postfix({'loss': train_loss/(pbar.n+1), 'acc': f"{train_acc:.2f}%"})

            # Evaluation
            test_acc = self.evaluate_model(model, test_loader, device)

            # Save best model
            if test_acc > best_acc:
                best_acc = test_acc
                # Store the best model state
                best_model_state = {k: v.clone() for k, v in model.state_dict().items()}

            # Print progress
            print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%")
            scheduler.step()

        # Load best model state
        model.load_state_dict(best_model_state)
        return best_acc

    def search(self, train_loader, test_loader, num_candidates=5, train_epochs=3):
        """Perform simplified neural architecture search for CPU"""
        print("Starting Neural Architecture Search...")
        device = 'cpu'  # Force CPU

        # Try models with increasing complexity
        for complexity in range(0, num_candidates):
            print(f"\nTesting architecture with complexity level {complexity}/{num_candidates-1}")

            # Generate tiny candidate architecture
            model = self.generate_tiny_architecture(complexity)
            model.to(device)

            # Estimate resource requirements
            model_size_kb = self.estimate_model_size(model)
            activation_size_kb = self.estimate_activation_size(model, self.input_size)

            print(f"Model Size: {model_size_kb:.2f} KB")
            print(f"Activation Size: {activation_size_kb:.2f} KB")

            # Check if model meets constraints
            if (model_size_kb > self.constraints['model_size_kb'] or
                activation_size_kb > self.constraints['activation_kb']):
                print("Model exceeds resource constraints, skipping...")
                continue

            # Train the model
            accuracy = self.train_model(model, train_loader, test_loader,
                                      epochs=train_epochs, device=device)

            # Update best model
            if self.best_model is None or accuracy > self.best_accuracy:
                self.best_accuracy = accuracy
                self.best_model = model
                print(f"New best model! Accuracy: {accuracy:.2f}%")

        print(f"\nNeural Architecture Search completed.")
        if self.best_model is not None:
            print(f"Best model accuracy: {self.best_accuracy:.2f}%")
            print(f"Best model size: {self.estimate_model_size(self.best_model):.2f} KB")
        else:
            print("No valid model found that meets the constraints.")

        return self.best_model

"""
Part 4: Run TinyNAS to search for an optimal architecture
=======================================================
"""

# Create TinyNAS instance
nas = TinyNAS(
    input_size=(3, 32, 32),  # CIFAR-10 image size
    num_classes=10,          # CIFAR-10 has 10 classes
    constraints=MODEL_CONSTRAINTS
)

# Run the search
best_model = nas.search(
    train_loader=train_loader,
    test_loader=test_loader,
    num_candidates=5,  # Try 5 different complexity levels
    train_epochs=2     # Use fewer epochs for faster search
)

"""
Part 5: Fine-tune the best model with CPU optimization
==============================
"""

if best_model is not None:
    print("Fine-tuning the best model...")

    # Use CPU for training
    device = 'cpu'
    best_model.to(device)

    # Setup for fine-tuning
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(best_model.parameters(), lr=0.001, weight_decay=1e-4)
    scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'max', patience=2, factor=0.5)

    # Training loop with early stopping
    best_acc = 0.0
    patience = 3
    patience_counter = 0
    epochs = 10  # Fewer epochs for CPU training

    for epoch in range(epochs):
        # Training
        best_model.train()
        train_loss = 0
        correct = 0
        total = 0

        pbar = tqdm(train_loader, desc=f"Fine-tuning Epoch {epoch+1}/{epochs}")
        for inputs, targets in pbar:
            inputs, targets = inputs.to(device), targets.to(device)

            # Forward pass
            outputs = best_model(inputs)
            loss = criterion(outputs, targets)

            # Backward pass and optimization
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()

            # Track metrics
            train_loss += loss.item()
            _, predicted = outputs.max(1)
            total += targets.size(0)
            correct += predicted.eq(targets).sum().item()

            # Update progress bar
            train_acc = 100.0 * correct / total
            pbar.set_postfix({'loss': train_loss/(pbar.n+1), 'acc': f"{train_acc:.2f}%"})

        # Evaluation
        best_model.eval()
        test_correct = 0
        test_total = 0

        with torch.no_grad():
            for inputs, targets in test_loader:
                inputs, targets = inputs.to(device), targets.to(device)
                outputs = best_model(inputs)
                _, predicted = outputs.max(1)
                test_total += targets.size(0)
                test_correct += predicted.eq(targets).sum().item()

        test_acc = 100.0 * test_correct / test_total

        # Update scheduler based on validation accuracy
        scheduler.step(test_acc)

        # Save best model and check for early stopping
        if test_acc > best_acc:
            best_acc = test_acc
            torch.save(best_model.state_dict(), 'best_model.pth')
            patience_counter = 0
            print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}% (best)")
        else:
            patience_counter += 1
            print(f"Epoch {epoch+1}: Train Acc: {train_acc:.2f}%, Test Acc: {test_acc:.2f}%")

        # Early stopping
        if patience_counter >= patience:
            print(f"Early stopping triggered after {epoch+1} epochs")
            break

    print(f"Final model accuracy: {best_acc:.2f}%")
    print("Model saved as 'best_model.pth'")
else:
    print("No valid model found that meets the constraints.")

"""
Part 6: Export the model for ESP32 WROVER (Simplified for CPU)
=======================================
"""

# Function to convert PyTorch model to C code directly
def convert_to_c_code(model, output_dir="esp32_model"):
    """Convert PyTorch model directly to C code for ESP32"""
    import os
    os.makedirs(output_dir, exist_ok=True)

    # Trace the model
    model.to('cpu')
    model.eval()
    example_input = torch.randn(1, 3, 32, 32)

    try:
        traced_model = torch.jit.trace(model, example_input)
        torch.jit.save(traced_model, f"{output_dir}/model_traced.pt")
        print(f"Model traced and saved to {output_dir}/model_traced.pt")

        # Get model parameters in a simple format
        model_params = []
        for name, param in model.named_parameters():
            if param.requires_grad:
                model_params.append((name, param.data.numpy().flatten()))

        # Calculate total size
        total_size = sum(param.size for _, param in model_params)

        # Create a header file
        with open(f"{output_dir}/model_data.h", "w") as f:
            f.write("""#ifndef MODEL_DATA_H
#define MODEL_DATA_H

#include <stdint.h>
#include <stdbool.h>
#include <stddef.h>

// Model input and output dimensions
#define MODEL_INPUT_HEIGHT   32
#define MODEL_INPUT_WIDTH    32
#define MODEL_INPUT_CHANNELS 3
#define MODEL_OUTPUT_SIZE    10

/**
 * Get the size of model weights in bytes
 */
size_t get_model_weights_size(void);

/**
 * Get the required activation buffer size in bytes
 */
size_t get_model_activation_size(void);

/**
 * Load model weights from flash to RAM
 * @param weights_buffer Pointer to pre-allocated weights buffer
 * @return true if successful
 */
bool load_model_weights(void* weights_buffer);

/**
 * Run inference using the loaded model
 * @param weights_buffer Pointer to weights buffer
 * @param activation_buffer Pointer to activation buffer
 * @param input_data Pointer to input data
 * @param output_data Pointer to output buffer
 * @return true if successful
 */
bool run_model_inference(void* weights_buffer, void* activation_buffer,
                         const uint8_t* input_data, float* output_data);

#endif // MODEL_DATA_H
""")

        # Create C file
        with open(f"{output_dir}/model_data.c", "w") as f:
            f.write(f"""#include "model_data.h"
#include <string.h>

// Total weights size: {total_size} bytes
#define MODEL_WEIGHTS_SIZE {total_size}
#define MODEL_ACTIVATION_SIZE {int(MODEL_CONSTRAINTS['activation_kb'] * 1024)}

size_t get_model_weights_size(void) {{
    return MODEL_WEIGHTS_SIZE;
}}

size_t get_model_activation_size(void) {{
    return MODEL_ACTIVATION_SIZE;
}}

bool load_model_weights(void* weights_buffer) {{
    // In a real implementation, this would load weights from flash
    memset(weights_buffer, 0, MODEL_WEIGHTS_SIZE);
    return true;
}}

bool run_model_inference(void* weights_buffer, void* activation_buffer,
                       const uint8_t* input_data, float* output_data) {{
    // In a real implementation, this would perform model inference
    memset(output_data, 0, MODEL_OUTPUT_SIZE * sizeof(float));
    return true;
}}
""")

        print(f"Generated C code in {output_dir}/")
        return output_dir

    except Exception as e:
        print(f"Error converting model: {e}")
        return None

# Export the model if we found one
if best_model is not None:
    # Generate C code directly
    c_code_dir = convert_to_c_code(best_model)

    if c_code_dir:
        # For Colab: create a zip file for download
        !zip -r esp32_model.zip {c_code_dir}

        # Download files
        from google.colab import files
        files.download('esp32_model.zip')
        files.download('best_model.pth')

print("MCUNet training and export completed!")